###### **<u>BERT</u>**
  </u>**(Pre-training of deep bidirectional transformers for language understanding)</u>**

###### *Introduction*

기존 GPT model: Transformer decoder를 이용하여 autoregressive 한 language modeling을 이용

BERT는 Transformer encoder(Bidirectional transformer) 이용하고 Embedding 하는것을 주요 task로 삼는다

**Input 을 위한 Embedding source**

1.**Token**-> word piece embedding 방식 사용. Char 단위로 embedding후 자주 등장하며 longgest 한 len의 sub-word 를 '하나의 단위'로 만든다. 자주 등장하지 않는 단어는 다시 sub-word 처리. 이전에 자주 등장하지 않았던 단어를 'OOV'처리하여 OOV problem도 어느정도 해결

2.**Segment**->Sentence Embedding, tokenizing한 단어들을 다시 하나의 sentence로 만드는 task. BERT에서는 두개의 sentence를 ([SEP]) 를 넣어 구분하고 다시 하나의 Segment로 지정하여 2개 이상의 입력이 필요한 case에 대응.

3.**Position** -> position encoding 을 통해 location information을 마킹한다



크게 2가지 task로 나눠서 설명하면 Pre-training -> Fine-tuning 단계로 나뉜다

**pre-training step**

1. MLM(Masked language modeling)

   - 순차적으로 mask하는게 아니라 sentence 일부분을 masking하여 prediction하는 방식으로 학습 -> 한 sentence 안에서만 학습하기때문에 추가적인 학습이 필요.

   - 기존의 input을 복구하는 일종의 denoising auto-encoding

   - input token의 일부분(15%)을 다음과 같이 처리한다

     1. 80%로 [Mask] token 적용

     2. 10%로 random token이나 원래의 token을 사용한다.

   - 단순히 mask token이나 random token만 사용하지 않고 원래 token을 사용하는 이유는 [Mask] token을 사용하지 않는 Fine-tuning step에서 방해가 될 수 있기 때문이다.
   - Result : 문맥을 파악하는 능력을 학습하게 된다는 실험적인 결과가 있다.
   - Pre-training 상황에서 overfitting을 방지하고 좀더 general한 information을 학습한다. fine-tuning단계에서는 [Mask]토큰이 활용되지 않고 정상적인 문장이 들어오기 때문이다. 
   -  Problem :[Mask] token이나 random token에 대해서 최적화가 되어버리면 반대로 이후 단계에서는 오히여 정상적인 input sentence를 처리하는데 문제가 될 수 있다.

2. NSP(Next sentence prediction)

   - A라는 sentence 다음에 이어질 sentence가 무엇인지 주어진 sentence(B,C)를 가지고 classification 하는 방법으로 학습
   - QA(Question Answering), NLI(Natural Language Inference)등 많은 downstream task들이 두 sentence 사이의 relation 을 학습시켜야 한다.
     - 하지만 기존의 일반적인 language model을 직접 이용할 수 없고, 해당 Task마다 따로 model을 구현해야 했었다 -> BERT에서는 NSP과정을 포함시키면서, 문장 사이의 relation을 파악하는 model도 같이 generalization 했다.
   - Binarized NSP(학습에서 입력할 두 문장을 선택할 때, 50%는 바로 다음에 오는 문장을, 50%는 관련 없는 문장을 선택해서 다음문장인지 아닌지를 분류)를 수행

   - Pre-training step에서는 BookCorput(800M), English Wikipedia(2500M, list, table, header를 제외한 text들만 사용) data를 sentence가 아닌 document level의 corpus로 사용.

**Fine-tuning**

- 입력에서 Pre-training 단계의 sentence pair (A,B) 는 Paraphrasing, Entailment의 전제-가설, QA 에서 질문-구절, Text classification이나 seq-tagging 에서의 degenerate 등의 분야에 대한 text pair과 비슷하게 구성
- 출력에서는 token representation들은 token level의 task에 대한 output layer에 보내고 [CLS] representation은 entailment나 sentiment analysis와 같이 classification에 대한 output layer로 보내서 multi-task를 시행
- 일반적으로 Pre-training에 비해서 cost가 저렴



Note

**ALBERT**

BERT같은 huge size model들은 cost, memory문제 이외에도 layer가 너무 많으면 성능이 저하되기도 한다

이를 개선하여 나온 모델이 ALBERT모델

Input Layer의 parameter 수를 줄여서 전체적인 model size를 줄인다

- Input token embedding size< In&out hidden size for each layer of transformer 로 제한을 걸면 모델 size가 커져도 성능이 떨어지지 않는다

Transformer의 각 layer 간 같은 Parameter를 sharing 하여 사용한다(Cross-layer parameter sharing)

이외에 더 큰 size로 pre-training을 한 뒤 여러가지 방법으로 NSP를 적용하여 BERT의 단점을 보완한 RoBERT Model이 있고 BERT 의 MLM pre-train 방법에서 original 한 값인지 파악하는 task가 아닌 replaced되 값인지 파악하는 추가적인 discriminator을 붙여서 파악하는 ELECTRA라는 Model이 있다 (GAN의 idea와 비슷) Note

**ALBERT**

BERT같은 huge size model들은 cost, memory문제 이외에도 layer가 너무 많으면 성능이 저하되기도 한다

이를 개선하여 나온 모델이 ALBERT모델

Input Layer의 parameter 수를 줄여서 전체적인 model size를 줄인다

- Input token embedding size< In&out hidden size for each layer of transformer 로 제한을 걸면 모델 size가 커져도 성능이 떨어지지 않는다

Transformer의 각 layer 간 같은 Parameter를 sharing 하여 사용한다(Cross-layer parameter sharing)

이외에 더 큰 size로 pre-training을 한 뒤 여러가지 방법으로 NSP를 적용하여 BERT의 단점을 보완한 RoBERT Model이 있고 BERT 의 MLM pre-train 방법에서 original 한 값인지 파악하는 task가 아닌 replaced되 값인지 파악하는 추가적인 discriminator을 붙여서 파악하는 ELECTRA라는 Model이 있다 (GAN의 idea와 비슷) 

